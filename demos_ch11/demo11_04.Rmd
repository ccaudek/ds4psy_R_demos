---
title: "Data science per psicologi - demo 11.04"
author: "Corrado Caudek"
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: readable
    highlight: pygments
    code_download: true
---

```{r}
suppressPackageStartupMessages({
  library("here")
  library("tidyverse")
  library("scales")
  library("bayesplot")
  library("prob")
  library("distrEx")
})

theme_set(bayesplot::theme_default(base_size = 12))
bayesplot::color_scheme_set("gray")
set.seed(84735)

knitr::opts_chunk$set(
  collapse = TRUE,
  tidy = 'styler',
  fig.width = 6,
  fig.asp = 0.618 # 1 / phi
)
```

# Propriet√† della covarianza 

Consideriamo le propriet√† della covarianza delle variabili casuali esaminando, per analogia, le stesse propriet√† quando le variabili vengono osservate in un campione di dati. Ricordiamo che le variabili casuali sono ignote prima dell'esecuzione dell'esperimento casuale. Una volta svolto l'esperimento casuale, le variabili casuali sono semplicemente un campione di osservazioni. Ovviamente, la nozione di variabile casuale fa riferimento a tutti i possibili campioni che si possono osservare.  Ma per semplicit√†, qui ne consideriamo uno soltanto.

Generiamo dei dati a caso per due variabili che chiamiamo $X$ e $Y$:

```{r}
set.seed(123)
n <- 20
x <- rnorm(n, 20, 3)
y <- x + rnorm(n, 0, 2)
```

Le due variabili sono associate:

```{r}
cor(x, y)
```

Un diagramma a dispersione si genera nel modo seguente:

```{r}
tibble(x, y) %>% 
  ggplot(aes(x, y)) +
  geom_point()
```

Esaminiamo le propriet√† della covarianza discusse nel capitolo 11 della dispensa.

## Varianza: la covarianza di una variabile con se stessa

Il modo pi√π semplice per ricordare che cos'√® la covarianza √® di pensare alla varianza come alla covarianza di una variabile con se stessa:

$$
\sigma_{xx} = \sum_i \big(x_i - \mathbb{E}(x)\big)\big(x_i - \mathbb{E}(x)\big) \cdot p_i(x)
$$
ovvero

```{r}
var(x)
cov(x, x)
```


## La covarianza tra una variabile aleatoria ùëã e una costante ùëê √® nulla

```{r}
cov(rep(3, n), x)
```

## La covarianza √® simmetrica

```{r}
cov(x, y) == cov(y, x)
```

## La correlazione non dipende dall‚Äôunit√† di misura

```{r}
cor(x, y)
cor(x*100, y*3)
```

## Covarianza tra ùëã e ùëå, ciascuna moltiplicata per una costante

Moltiplichiamo $X$ per 3 e $Y$ per 2 e calcoliamo la nuova covarianza:

```{r}
cov(x, y)
cov(3*x, 2*y)
6 * cov(x, y)
```

## La varianza di una somma

Sommiamo $X$ e $Y$ e calcoliamo la varianza:

```{r}
z <- x + y
z
```

La varianza di $Z$ √®

```{r}
var(z)
```

ovvero

```{r}
var(x) + var(y) + 2*cov(x,y)
```

Consideriamo 3 variabili:

```{r}
w <- y + rnorm(n, 0, 4)
```

Esaminiamo la matrice di correlazioni:

```{r}
cor(cbind(x, y, w)) %>% 
  round(2)
```

Creiamo una nuova variabile casuale, $T$, nel modo seguente: $T = X + Y + Z$. 
```{r}
t <- x + y + w
t
```

Calcoliamo la varianza di $T$:

```{r}
var(t)
```

ovvero

```{r}
var(x) + var(y) + var(w) + 2*cov(x, y) + 2*cov(x, w) + 2*cov(w, y) 
```

## Conclusione {-}

Si noti che, in tutte le dimostrazioni prevedenti, ho usato le funzioni di $\mathsf{R}$ `var()` e `cov()`, le quali hanno $n-1$ al denominatore. Se facciamo riferimento alle variabili casuali e alla varianza e alla covarianza delle variabili casuali, NON bisogna usare $n-1$ al denominatore. In questo demo ho usato le funzioni `var()` e `cov()` solo per semplicit√†, per illustrare la logica che sta alla base delle propriet√† della varianza e della covarianza delle variabili casuali. Se vogliamo i risultati numerici esatti, dobbiamo, per gli esempi presenti, usare $n$ al denominatore delle formule della varianza e della covarianza.


## Informazioni sulla sessione di lavoro

```{r}
utils::sessionInfo()
```


