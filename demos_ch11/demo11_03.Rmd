---
title: "Data science per psicologi - demo 11.02"
author: "Corrado Caudek"
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: readable
    highlight: pygments
    code_download: true
---

```{r}
suppressPackageStartupMessages({
  library("here")
  library("tidyverse")
  library("scales")
  library("bayesplot")
  library("prob")
  library("distrEx")
})

theme_set(bayesplot::theme_default(base_size = 12))
bayesplot::color_scheme_set("gray")
set.seed(84735)

knitr::opts_chunk$set(
  collapse = TRUE,
  tidy = 'styler',
  fig.width = 6,
  fig.asp = 0.618 # 1 / phi
)
```

# La covarianza di due variabili aleatorie discrete

Poniamoci il problema di calcolare la covarianza e la correlazione di due variabili aleatorie discrete utilizzando l'informazione fornita dalla distribuzione di probabilità congiunta.  

Consideriamo l'esperimento casuale che corrisponde al lancio di due dadi bilanciati. Creiamo lo spazio campionario di questo esperimento casuale e definiamo su di esso le seguenti variabili aleatorie:

- U: somma dei punti dei due dadi,
- V: 1 se i punti del lancio del primo dado sono minori di 4, 0 altrimenti.

Possiamo ottenere un elenco di tutti gli eventi elementari dello spazio campionario usando le istruzioni seguenti:

```{r}
S <- rolldie(2, nsides = 6, makespace = TRUE)
S <- addrv(S, U = X1+X2, V = ifelse(X1 < 4, 1, 0))
S
```

Le istruzioni precedenti specificano, in corrispondenza di ogni punto dello spazio campionario (ovvero, in ciascuna riga del data.frame che viene generato), il valore assunto dalle due variabili aleatorie che sono state definite.

La descrizione dello spazio campionario fornita da `R` è corretta: questo esperimento casuale produce infatti 6 $\times$ 6 esiti (eventi elementari) possibili. Ma si noti che alcune righe del data.frame si ripetono più volte. Possiamo dunque semplificare tale descrizione nel modo seguente:

```{r}
UV <- marginal(S, vars = c("U", "V")) 
UV
```

L'elenco dei valori che le due variabili aleatorie $U$ e $V$ possono assumere, insieme alla probabilità del loro verificarsi, costituisce la _distribuzione di probabilità congiunta_ delle due variabili aleatorie. È anche possibile presentare la distribuzione di probabilità congiunta in forma tabulare. A tale fine possiamo utilizzare la funzione `xtabs()`:

```{r}
xtabs(round(probs, 3) ~ U + V, data = UV)
```

Ripetiamo ora il processo precedente chiedendo alla funzione `marginal()` di calcolare le due distribuzioni marginali univariate:

```{r}
pu <- marginal(S, vars = "U")
pu
```

e

```{r}
pv <- marginal(S, vars = "V")
pv
```

In maniera equivalente, lo stesso risultato si trova applicando le funzioni `rowSums()` e `colSums()` all'oggetto creato da `xtabs()`:

```{r}
temp <- xtabs(probs ~ U + V, data = UV)
rowSums(temp)
```

e

```{r}
colSums(temp)
```

## Valore atteso

Calcoliamo ora il valore atteso delle due variabili aleatorie. Utilizziamo la funzione `DiscreteDistribution()`. Il primo argomento richiede un vettore che specifica il supporto della variabile casuale discreta. Il secondo argomento è un vettore che, per ciasun valore della variabile casuale discreta, specifica il corrispondente valore della funzione di massa di probabilità. Mediante la funzione `DiscreteDistribution()` otteniamo il valore atteso di `U`:

```{r}
U <- DiscreteDistribution(supp = pu$U, prob = pu$probs)
mu_u <- E(U)
mu_u
```

Inoltre, il valore atteso di `V` è:

```{r}
V <- DiscreteDistribution(supp = pv$V, prob = pv$probs)
mu_v <- E(V)
mu_v
```


## Covarianza

Utilizzando le informazioni precedenti possiamo calcolare la covarianza tra $U$ e $V$. La formula della covarianza

$$
\sigma_{xy} = \sum_i \big(x_i - \mathbb{E}(x)\big)\big(y_i - \mathbb{E}(y)\big) \cdot p_i(x,y)
$$

può essere implementata in `R` nel modo seguente:

```{r}
s_uv <- sum((UV$U - mu_u) * (UV$V - mu_v) * UV$probs)
s_uv
```

Lo stesso risultato si ottiene usando la formula alternativa per il calcolo della covarianza:

$$
\sigma_{xy} = \mathbb{E}(xy) -\mathbb{E}(x)\mathbb{E}(y)
$$
ovvero 

```{r}
sum((UV$U * UV$V) * UV$probs) - mu_u * mu_v
```


## Correlazione

Conoscendo la covarianza è possibile calcolare la correlazione. La correlazione infatti è una covarianza standardizzata:

$$
\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}
$$

Per standardizzare dobbiamo dividere per le due deviazioni standard. Iniziamo dunque a calcolare le varianze delle due variabili aleatorie $U$ e $V$:

```{r}
v_u <- sum((pu$U - mu_u)^2 * pu$probs)
v_u
v_v <- sum((pv$V - mu_v)^2 * pv$probs)
v_v
```

oppure, in maniera equivalente

```{r}
var(U)
var(V)
```

Abbiamo ora tutte le informazioni necessarie per il calcolo della correlazione:

```{r}
s_uv / sqrt(var(U) * var(V))
```

## Covarianza (2)

Calcoliamo nuovamente la covarianza in maniera più diretta, ovvero applicando la formula seguente mediante due cicli `for()`:

$$
\sigma_{xy} = \sum_i \big(x_i - \mathbb{E}(x)\big)\big(y_i - \mathbb{E}(y)\big) \cdot p_i(x,y)
$$
Iniziamo creando un oggetto chiamato `puv` che contiene la distribuzione di probabilità congiunta delle variabili $U$ e $V$:

```{r}
puv <- xtabs(round(probs, 3) ~ U + V, data = UV)
puv
```

La somma degli elementi di `puv` deve essere uguale a 1 (è sempre una buona idea controllare):

```{r}
sum(puv)
```

La variabile casuale `u` assume i valori:

```{r}
u <- 2:12
u
```

La variabile casuale `v` assume i valori:

```{r}
v <- 0:1
v
```

Abbiamo già calcolato in precedenza i valori attesi delle due variabili casuali, che sono chiamati `mu_u` e `mu_v`:

```{r}
mu_u
mu_v
```

Per semplicità, creo una matrice vuota delle stesse dimensioni dell'oggetto `puv`:

```{r}
suv = matrix(, nrow = 11, ncol = 2) # empty matrix
suv
```

Utilizzo due cicli `for()` nidificati (_nested_) per calcolare il valore di ciascun prodotto 

$$
\big(u_i - \mathbb{E}(u)\big)\big(v_j - \mathbb{E}(v)\big) \cdot p_{i,j}(u,v)
$$ 

con $i \in \{1, \dots, 11\}$ e $j \in \{1, 2\}$, che verrà salvato nel corrispondente elemento della matrice `suv`:

```{r}
for (i in 1:11) 
  for (j in 1:2) {
    suv[i, j] <- (u[i] - mu_u) * (v[j] - mu_v) * puv[i, j]
  }
```

Nel caso presente, si inizia con `i = 1`. Viene quindi svolto il ciclo interno e quindi, in due esecuzioni, `j = 1` e `j = 2`. Una volta completato il ciclo interno, il valore dell'indice del ciclo esterno varrà `i = 2`. Si ripetono le esecuzioni del ciclo interno, `j = 1` e `j = 2`. Così via fino a `i = 11`. 

Esaminiamo la prima esecuzione del ciclo interno per `i = 1`. Ciò corrisponde a:

```{r, eval=FALSE}
(u[1] - mu_u) * (v[1] - mu_v) * puv[1, 1]
```

ovvero

$$
\big(u_1 - \mathbb{E}(u)\big)\big(v_1 - \mathbb{E}(v)\big) \cdot p_{1,1}(u,v)
$$
Nel secondo step avremo 

```{r, eval=FALSE}
(u[1] - mu_u) * (v[2] - mu_v) * puv[1, 2]
```

ovvero

$$
\big(u_1 - \mathbb{E}(u)\big)\big(v_2 - \mathbb{E}(v)\big) \cdot p_{1,2}(u,v)
$$
E così via per tutti gli $11\times2$ elementi della matrice che contiene la distribuzione di probabilità congiunta. 

È irrilevante quale sia il ciclo esterno e quale quello interno, questo cambia soltanto l'ordine con il quale ventono svolte le operazioni.

Una volta svolti i calcoli precedenti, non resta che sommare tutti gli addendi, ovvero gli $11\times2$ elementi della matrice `suv`. Così facendo troviamo la covarianza tra $U$ e $V$:

```{r}
sum(suv)
```

Il risultato riproduce quello trovato in precedenza.


## Informazioni sulla sessione di lavoro

```{r}
utils::sessionInfo()
```


