---
title: "Data science per psicologi - demo 11.02"
author: "Corrado Caudek"
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: readable
    highlight: pygments
    code_download: true
---

```{r}
suppressPackageStartupMessages({
  library("here")
  library("tidyverse")
  library("scales")
  library("bayesplot")
  library("prob")
  library("distrEx")
})

theme_set(bayesplot::theme_default(base_size = 12))
bayesplot::color_scheme_set("gray")
set.seed(84735)

knitr::opts_chunk$set(
  collapse = TRUE,
  tidy = 'styler',
  fig.width = 6,
  fig.asp = 0.618 # 1 / phi
)
```

# La covarianza di due variabili aleatorie discrete

Poniamoci il problema di calcolare la covarianza e la correlazione di due variabili aleatorie discrete utilizzando l'informazione fornita dalla *distribuzione di probabilità congiunta*.  

Consideriamo l'esperimento casuale che corrisponde al lancio di due dadi bilanciati. Creiamo lo spazio campionario di questo esperimento casuale e definiamo su di esso le seguenti variabili aleatorie:

- U: somma dei punti dei due dadi,
- V: 1 se i punti del lancio del primo dado sono minori di 4, 0 altrimenti.

Creiamo lo spazio campione nel modo seguente:

```{r}
S <- rolldie(2, nsides = 6, makespace = TRUE)
S
```
Su tale spazio campione definisco le due variabili casuali descritte in precedenza:

```{r}
S <- addrv(S, U = X1+X2, V = ifelse(X1 < 4, 1, 0))
S
```

Le istruzioni precedenti specificano, in corrispondenza di ogni evento elementare dello spazio campione -- ovvero, in corrispondenza di ciascuna riga del DataFrame -- il valore assunto dalle due variabili casuali. 

Possiamo semplificare tale descrizione usando `prob::marginal()`. Si noti la funzione `c()` che consente di specificare un vettore che contiene il nome delle due variabili:

```{r}
UV <- marginal(S, vars = c("U", "V")) 
UV
```

L'elenco dei valori che le due variabili casuali $U$ e $V$ possono assumere, insieme alla probabilità del loro verificarsi, costituisce la *distribuzione di probabilità congiunta*. Verifico che questa sia veramente una distribuzione di probabilità congiunta:

```{r}
sum(UV$probs)
```

È anche possibile presentare la distribuzione di probabilità congiunta in forma tabulare. A tale fine possiamo utilizzare la funzione `xtabs()`:

```{r}
xtabs(probs ~ U + V, data = UV) %>% 
  round(3)
```

Calcoliamo ora le due distribuzioni marginali. Per la variabile `U` otteniamo:

```{r}
pu <- marginal(S, vars = "U")
pu
```

e per la variabile `V` otteniamo:

```{r}
pv <- marginal(S, vars = "V")
pv
```

In maniera equivalente, lo stesso risultato si trova applicando le funzioni `rowSums()` e `colSums()` all'oggetto creato da `xtabs()`:

```{r}
temp <- xtabs(probs ~ U + V, data = UV)
rowSums(temp)
```

e

```{r}
colSums(temp)
```

## Valore atteso

Calcoliamo il valore atteso delle due variabili casuali. Utilizziamo la funzione `DiscreteDistribution()`. Il primo argomento richiede un vettore che specifica il supporto della variabile casuale discreta; il secondo argomento è un vettore che specifica la funzione di massa di probabilità. Il valore atteso di `U` è:

```{r}
U <- DiscreteDistribution(supp = pu$U, prob = pu$probs)
mu_u <- E(U)
mu_u
```

Inoltre, il valore atteso di `V` è:

```{r}
V <- DiscreteDistribution(supp = pv$V, prob = pv$probs)
mu_v <- E(V)
mu_v
```


## Covarianza

Possiamo ora calcolare la covarianza tra $U$ e $V$. La formula della covarianza

$$
\sigma_{xy} = \sum_i \big[x_i - \mathbb{E}(x)\big]\big[y_i - \mathbb{E}(y)\big] \cdot p_i(x,y)
$$

può essere implementata in $\mathsf{R}$ nel modo seguente:

```{r}
s_uv <- sum((UV$U - mu_u) * (UV$V - mu_v) * UV$probs)
s_uv
```

Lo stesso risultato si ottiene usando la formula alternativa per il calcolo della covarianza:

$$
\sigma_{xy} = \mathbb{E}(xy) -\mathbb{E}(x)\mathbb{E}(y)
$$
ovvero 

```{r}
sum((UV$U * UV$V) * UV$probs) - mu_u * mu_v
```


## Correlazione

Conoscendo la covarianza è possibile calcolare la correlazione. La correlazione infatti è una covarianza standardizzata:

$$
\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}
$$

Per standardizzare dobbiamo dividere per le due deviazioni standard. Iniziamo dunque a calcolare le varianze delle due variabili casuali $U$ e $V$:

```{r}
v_u <- sum((pu$U - mu_u)^2 * pu$probs)
v_u
v_v <- sum((pv$V - mu_v)^2 * pv$probs)
v_v
```

oppure, in maniera equivalente

```{r}
var(U)
var(V)
```

Abbiamo ora tutte le informazioni necessarie per il calcolo della correlazione:

```{r}
s_uv / sqrt(var(U) * var(V))
```

## Covarianza (2)

Calcoliamo nuovamente la covarianza in maniera più diretta, ovvero applicando la formula seguente mediante due cicli `for()`:

$$
\sigma_{xy} = \sum_i \big(x_i - \mathbb{E}(x)\big)\big(y_i - \mathbb{E}(y)\big) \cdot p_i(x,y)
$$
Iniziamo creando un oggetto chiamato `puv` che contiene la distribuzione di probabilità congiunta delle variabili $U$ e $V$:

```{r}
puv <- xtabs(probs ~ U + V, data = UV)
puv
```

La somma degli elementi di `puv` deve essere uguale a 1 (è sempre una buona idea controllare):

```{r}
sum(puv)
```

La variabile casuale `u` assume i valori:

```{r}
u <- 2:12
u
```

La variabile casuale `v` assume i valori:

```{r}
v <- 0:1
v
```

Abbiamo già calcolato in precedenza i valori attesi delle due variabili casuali, che sono chiamati `mu_u` e `mu_v`:

```{r}
mu_u
mu_v
```

Per semplicità, creo una matrice vuota delle stesse dimensioni dell'oggetto `puv`:

```{r}
suv = matrix(, nrow = 11, ncol = 2) # empty matrix
suv
```

Utilizzo due cicli `for()` nidificati (_nested_) per calcolare il valore di ciascun prodotto 

$$
\big(u_i - \mathbb{E}(u)\big)\big(v_j - \mathbb{E}(v)\big) \cdot p_{i,j}(u,v)
$$ 

con $i \in \{1, \dots, 11\}$ e $j \in \{1, 2\}$, che verrà salvato nel corrispondente elemento della matrice `suv`:

```{r}
for (i in 1:11) 
  for (j in 1:2) {
    suv[i, j] <- (u[i] - mu_u) * (v[j] - mu_v) * puv[i, j]
  }
```

Nel caso presente, si inizia con `i = 1`. Viene quindi svolto il ciclo interno e quindi, in due esecuzioni, `j = 1` e `j = 2`. Una volta completato il ciclo interno, il valore dell'indice del ciclo esterno varrà `i = 2`. Si ripetono le esecuzioni del ciclo interno, `j = 1` e `j = 2`. Così via fino a `i = 11`. 

Esaminiamo la prima esecuzione del ciclo interno per `i = 1`. Ciò corrisponde a:

```{r, eval=FALSE}
(u[1] - mu_u) * (v[1] - mu_v) * puv[1, 1]
```

ovvero

$$
\big(u_1 - \mathbb{E}(u)\big)\big(v_1 - \mathbb{E}(v)\big) \cdot p_{1,1}(u,v)
$$
Nel secondo step avremo 

```{r, eval=FALSE}
(u[1] - mu_u) * (v[2] - mu_v) * puv[1, 2]
```

ovvero

$$
\big(u_1 - \mathbb{E}(u)\big)\big(v_2 - \mathbb{E}(v)\big) \cdot p_{1,2}(u,v)
$$
E così via per tutti gli $11\times2$ elementi della matrice che contiene la distribuzione di probabilità congiunta. 

È irrilevante quale sia il ciclo esterno e quale quello interno, questo cambia soltanto l'ordine con il quale ventono svolte le operazioni.

Una volta svolti i calcoli precedenti, non resta che sommare tutti gli addendi, ovvero gli $11\times2$ elementi della matrice `suv`. Così facendo troviamo la covarianza tra $U$ e $V$:

```{r}
sum(suv)
```

Il risultato riproduce quello trovato in precedenza.


## Informazioni sulla sessione di lavoro

```{r}
utils::sessionInfo()
```


